# wheredamilk ü•õ

> Real-time assistive vision ‚Äî **find items** and **read labels** by speaking, guided by AI-powered depth and voice guidance.

---

## ‚úÖ What's Done

### Core Pipeline
- [x] **YOLOv8n object detection** ‚Äî real-time, 640√ó480, every 2nd frame (`vision/yolo.py`)
- [x] **PaddleOCR text reading** ‚Äî crops top-1/2 boxes by confidence, reads label text (`vision/ocr.py`)
- [x] **MiDaS monocular depth** ‚Äî real depth from a single RGB webcam, no depth camera needed (`vision/depth.py`)
- [x] **Keyword matching** ‚Äî case-insensitive substring, e.g. "milk" in "DairyPure Whole Milk" (`logic/match.py`)
- [x] **Spatial direction** ‚Äî left/right/ahead from bbox centre + MiDaS depth (bbox-area fallback) (`logic/direction.py`)
- [x] **IoU tracker** ‚Äî locks onto target, tracks across frames, handles short occlusions (`logic/tracker.py`)

### Voice & Audio
- [x] **ElevenLabs TTS** üéôÔ∏è ‚Äî natural, human-quality voice via `eleven_turbo_v2` model (`utils/tts.py`)
- [x] **pyttsx3 fallback** ‚Äî offline TTS if `ELEVEN_API_KEY` not set
- [x] **Throttled speech** ‚Äî speaks only on direction change or every ~1s (no spam)
- [x] **`.env` support** ‚Äî API key loaded automatically via `python-dotenv`
- [x] **Continuous mic listener** ‚Äî background thread, always listening (`utils/speech.py`)
- [x] **Voice commands** ‚Äî "find milk", "read", "stop", "quit"

### Modes
- [x] **Find mode** ‚Äî YOLO ‚Üí OCR top boxes ‚Üí match ‚Üí lock ‚Üí track ‚Üí speak directions continuously
- [x] **Read mode** ‚Äî OCR largest box ‚Üí speak label text once

### App
- [x] **`main.py`** ‚Äî fully voice-controlled webcam loop, OpenCV overlay
- [x] **`app.py`** ‚Äî optional Flask REST API (`/find`, `/read`, `/status`)

---

## üîú To-Do

### Accuracy & Robustness
- [ ] **Re-lock after occlusion** ‚Äî if tracker loses target entirely, re-trigger OCR search
- [ ] **Multi-target disambiguation** ‚Äî two matching items visible ‚Üí pick closer one via MiDaS
- [ ] **Confidence-gated OCR** ‚Äî skip OCR if YOLO confidence < threshold
- [ ] **Vertical guidance** ‚Äî "look higher / lower / bottom shelf"

### User Experience
- [ ] **Audio-only mode** ‚Äî suppress OpenCV window for real device use
- [ ] **Low-power mode** ‚Äî skip MiDaS, use bbox-area only

### Testing (live ‚Äî requires webcam + deps)
- [ ] Run `main.py`, confirm YOLO boxes appear
- [ ] Test "find milk" with a printed label
- [ ] Test "read" mode on product packaging
- [ ] Confirm ElevenLabs voice fires on startup phrase
- [ ] Confirm TTS throttle ‚Äî no speech spam

### Platform / Deployment
- [ ] iOS / Android app ‚Üí calls Flask `/find` and `/read`

---

## Architecture

```
Voice Command ("find milk")
        ‚Üì
Mic Listener (background thread)    utils/speech.py
        ‚Üì
Webcam (OpenCV 640√ó480)
        ‚Üì
YOLOv8n ‚Äî detect objects            vision/yolo.py
        ‚Üì
MiDaS ‚Äî estimate depth              vision/depth.py
        ‚Üì
PaddleOCR ‚Äî read text on crop       vision/ocr.py
        ‚Üì
Keyword match                       logic/match.py
        ‚Üì
IoU Tracker ‚Äî lock target           logic/tracker.py
        ‚Üì
Direction (left/right + depth)      logic/direction.py
        ‚Üì
ElevenLabs TTS üéôÔ∏è (throttled)       utils/tts.py
```

---

## Project Structure

```
wheredamilk/
‚îú‚îÄ‚îÄ .env                 ‚Üê API keys (gitignored, never pushed)
‚îú‚îÄ‚îÄ main.py              ‚Üê voice-controlled webcam loop
‚îú‚îÄ‚îÄ app.py               ‚Üê Flask REST API (optional)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ vision/
‚îÇ   ‚îú‚îÄ‚îÄ yolo.py          ‚Üê YOLOv8n detector
‚îÇ   ‚îú‚îÄ‚îÄ ocr.py           ‚Üê PaddleOCR wrapper
‚îÇ   ‚îî‚îÄ‚îÄ depth.py         ‚Üê MiDaS monocular depth
‚îÇ
‚îú‚îÄ‚îÄ logic/
‚îÇ   ‚îú‚îÄ‚îÄ match.py         ‚Üê keyword matching
‚îÇ   ‚îú‚îÄ‚îÄ direction.py     ‚Üê left/right/ahead + real depth
‚îÇ   ‚îî‚îÄ‚îÄ tracker.py       ‚Üê IoU single-target tracker
‚îÇ
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ tts.py           ‚Üê ElevenLabs TTS + pyttsx3 fallback
    ‚îî‚îÄ‚îÄ speech.py        ‚Üê continuous mic listener
```

---

## Installation

```bash
cd /Users/athulyaanil/wheredamilk
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# macOS mic support
brew install portaudio && pip install pyaudio

# MiDaS weights (~400 MB) download automatically on first run
```

---

## ElevenLabs Setup üéôÔ∏è

1. Sign up at **[elevenlabs.io](https://elevenlabs.io)** (free ‚Äî 10,000 chars/month)
2. Go to **Settings ‚Üí API Keys** ‚Üí create and copy your key
3. Add to `.env` in the project root:

```bash
ELEVEN_API_KEY=sk_your_key_here
ELEVEN_VOICE_ID=Rachel        # optional ‚Äî Rachel is default
```

> The `.env` file is gitignored and **never pushed to GitHub**.
> If no key is set, the app falls back to pyttsx3 (offline, robotic voice).

---

## Usage

### Run

```bash
python main.py
```

| Say | What happens |
|---|---|
| `"find milk"` | Scans scene, locks on milk, speaks live directions |
| `"find orange juice"` | Works for any item name |
| `"read"` / `"what is this"` | OCRs biggest box, speaks the label once |
| `"stop"` / `"cancel"` | Return to idle |
| `"quit"` / `"exit"` | Close app |

Press `q` in the OpenCV window to also quit.

### Flask API (optional)

```bash
python app.py

curl -X POST http://localhost:5000/find -d '{"query":"milk"}' -H 'Content-Type: application/json'
curl -X POST http://localhost:5000/read
curl http://localhost:5000/status
```

---

## Tech Stack

| Library | Purpose |
|---|---|
| `ultralytics` | YOLOv8n detection |
| `opencv-python` | Webcam + drawing |
| `paddleocr` | Text recognition |
| `transformers` + `timm` | MiDaS depth model |
| `elevenlabs` | üéôÔ∏è Natural TTS (primary) |
| `pyttsx3` | Offline TTS fallback |
| `SpeechRecognition` | Mic voice commands |
| `python-dotenv` | `.env` key loading |
| `flask` | Optional REST API |

---

*"wheredamilk ‚Äî real-time navigation and label reading for blind users using object detection, depth estimation, and natural voice guidance."*
