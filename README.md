# wheredamilk ğŸ¥›

> Real-time assistive vision â€” **find items** and **read labels** by speaking, guided by AI-powered depth and voice guidance.

---

## âœ… What's Working

### Core Pipeline
- [x] **YOLOv8n object detection** â€” real-time, 640Ã—480, every 2nd frame (`vision/yolo.py`)
- [x] **EasyOCR text reading** â€” high-quality OCR comparable to Google ML Kit, crops top-1/2 boxes by confidence, reads label text (`vision/ocr.py`)
- [x] **MiDaS monocular depth** â€” real depth from a single RGB webcam, no depth camera needed (`vision/depth.py`)
- [x] **Keyword matching** â€” case-insensitive substring, e.g. "milk" in "DairyPure Whole Milk" (`logic/match.py`)
- [x] **Spatial direction** â€” left/right/ahead from bbox centre + MiDaS depth (bbox-area fallback) (`logic/direction.py`)
- [x] **IoU tracker** â€” locks onto target, tracks across frames, handles short occlusions (`logic/tracker.py`)
- [x] **ElevenLabs TTS** ğŸ™ï¸ â€” natural, human-quality voice via `eleven_turbo_v2` model, with **edge-tts fallback** (Microsoft Edge voices, no API key needed) (`utils/tts.py`)
- [x] **Throttled speech** â€” speaks only on direction change or every ~1s (no spam)
- [x] **Continuous mic listener** â€” background thread, always listening (`utils/speech.py`)
- [x] **Voice commands** â€” "find milk", "read", "stop", "quit"
- [x] **Find mode** â€” YOLO â†’ OCR top boxes â†’ match â†’ lock â†’ track â†’ speak directions continuously
- [x] **Read mode** â€” OCR largest box â†’ speak label text once
- [x] **`main.py`** â€” fully voice-controlled webcam loop, OpenCV overlay
- [x] **`app.py`** â€” optional Flask REST API (`/find`, `/read`, `/status`)

---

## ğŸ”œ Needs Work

- [ ] **End-to-end live testing** â€” run `main.py` with webcam, verify full pipeline
- [ ] **Re-lock after occlusion** â€” if tracker loses target entirely, re-trigger OCR search
- [ ] **Multi-target disambiguation** â€” two matching items visible â†’ pick closer one via MiDaS
- [ ] **Confidence-gated OCR** â€” skip OCR if YOLO confidence < threshold
- [ ] **Vertical guidance** â€” "look higher / lower / bottom shelf"
- [ ] **iOS / Android app** â†’ calls Flask `/find` and `/read`

---

## Architecture

```
Voice Command ("find milk")
        â†“
Mic Listener (background thread)    utils/speech.py
        â†“
Webcam (OpenCV 640Ã—480)
        â†“
YOLOv8n â€” detect objects            vision/yolo.py
        â†“
MiDaS â€” estimate depth              vision/depth.py
        â†“
EasyOCR â€” read text on crop         vision/ocr.py
        â†“
Keyword match                       logic/match.py
        â†“
IoU Tracker â€” lock target           logic/tracker.py
        â†“
Direction (left/right + depth)      logic/direction.py
        â†“
ElevenLabs TTS ğŸ™ï¸ (throttled)       utils/tts.py
```

---

## Project Structure

```
wheredamilk/
â”œâ”€â”€ .env                 â† API keys (gitignored, never pushed)
â”œâ”€â”€ main.py              â† voice-controlled webcam loop
â”œâ”€â”€ app.py               â† Flask REST API (optional)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ vision/
â”‚   â”œâ”€â”€ yolo.py          â† YOLOv8n detector
â”‚   â”œâ”€â”€ ocr.py           â† EasyOCR wrapper
â”‚   â””â”€â”€ depth.py         â† MiDaS monocular depth
â”‚
â”œâ”€â”€ logic/
â”‚   â”œâ”€â”€ match.py         â† keyword matching
â”‚   â”œâ”€â”€ direction.py     â† left/right/ahead + real depth
â”‚   â””â”€â”€ tracker.py       â† IoU single-target tracker
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ tts.py           â† ElevenLabs TTS (throttled)
    â””â”€â”€ speech.py        â† continuous mic listener
```

---

## Installation

```bash
cd /Users/balachandrads/Desktop/Projects/wheredamilk
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# macOS mic support (optional)
brew install portaudio && pip install pyaudio

# EasyOCR weights (~70 MB) and MiDaS weights (~400 MB) download automatically on first run
```

> **Note:** EasyOCR may take 30-60 seconds to initialize on first run as it downloads the model.

---

## Text Recognition (EasyOCR) âœ¨

**EasyOCR** provides high-quality text recognition comparable to Google ML Kit. No setup needed â€” works automatically.

> EasyOCR downloads its model (~70 MB) automatically on first run. This may take 30-60 seconds.

---

## Text-to-Speech (TTS) ğŸ™ï¸

### Primary: ElevenLabs (Optional)

For premium natural voices via **ElevenLabs**:

1. Sign up at **[elevenlabs.io](https://elevenlabs.io)** (free â€” 10,000 chars/month)
2. Go to **Settings â†’ API Keys** â†’ create and copy your key
3. Add to `.env` in the project root:

```bash
ELEVEN_API_KEY=sk_your_key_here
ELEVEN_VOICE_ID=AeRdCCKzvd23BpJoofzx  # optional â€” Rachel is default
```

> The `.env` file is gitignored and **never pushed to GitHub**.

### Fallback: edge-tts (Built-in) âœ¨ 

If **no ElevenLabs key is set**, the app automatically falls back to **edge-tts** â€” Microsoft Edge's high-quality voices. **No API key needed!** Just works.

- **Fallback chain:** ElevenLabs â†’ edge-tts â†’ afplay (system speaker)
- **Best for:** Users without API keys, offline environments (sort of)
- **Quality:** Comparable to Google ML Kit voices

---

## Usage

### Run

```bash
python main.py
```

| Say | What happens |
|---|---|
| `"find milk"` | Scans scene, locks on milk, speaks live directions |
| `"find orange juice"` | Works for any item name |
| `"read"` / `"what is this"` | OCRs biggest box, speaks the label once |
| `"stop"` / `"cancel"` | Return to idle |
| `"quit"` / `"exit"` | Close app |

Press `q` in the OpenCV window to also quit.

### Flask API (optional)

```bash
python app.py

curl -X POST http://localhost:5000/find -d '{"query":"milk"}' -H 'Content-Type: application/json'
curl -X POST http://localhost:5000/read
curl http://localhost:5000/status
```

---

## Tech Stack

| Library | Purpose |
|---|---|
| `ultralytics` | YOLOv8n detection |
| `opencv-python` | Webcam + drawing |
| `easyocr` | Text recognition (ML Kit-quality) |
| `torch` | PyTorch (required by EasyOCR) |
| `transformers` + `timm` | MiDaS depth model |
| `elevenlabs` | ğŸ™ï¸ Premium TTS |
| `edge-tts` | ğŸ™ï¸ Fallback TTS (no API key) |
| `SpeechRecognition` | Mic voice commands |
| `python-dotenv` | `.env` key loading |
| `flask` | Optional REST API |

---

*"wheredamilk â€” real-time navigation and label reading for blind users using object detection, depth estimation, and natural voice guidance."*
