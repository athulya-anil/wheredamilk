# wheredamilk 

> Real-time assistive vision â€” **find items** and **read labels** by speaking, guided by AI-powered depth and direction audio.

---

## âœ… What's Done

### Core Pipeline
- [x] **YOLOv8n object detection** â€” real-time, 640Ã—480, every 2nd frame (`vision/yolo.py`)
- [x] **PaddleOCR text reading** â€” crops top-1/2 boxes by confidence, reads label text (`vision/ocr.py`)
- [x] **MiDaS monocular depth** â€” estimates real depth from single RGB webcam, no depth camera needed (`vision/depth.py`)
- [x] **Keyword matching** â€” case-insensitive substring, e.g. "milk" in "DairyPure Whole Milk" (`logic/match.py`)
- [x] **Spatial direction** â€” left/right/ahead from bbox centre + real MiDaS depth (bbox-area fallback) (`logic/direction.py`)
- [x] **IoU tracker** â€” locks onto target, tracks across frames, handles short occlusions (`logic/tracker.py`)

### Audio
- [x] **ElevenLabs TTS** â€” natural voice guidance via `eleven_turbo_v2` (`utils/tts.py`)
- [x] **pyttsx3 fallback** â€” offline TTS if ElevenLabs key not set
- [x] **Throttled speech** â€” speaks only on direction change or every ~1s (no spam)
- [x] **Voice command input** â€” continuous mic listener in background thread (`utils/speech.py`)
- [x] **Command parsing** â€” "find milk", "read", "stop", "quit"

### Modes
- [x] **Find mode** â€” YOLO â†’ OCR top boxes â†’ match â†’ lock â†’ track â†’ speak directions continuously
- [x] **Read mode** â€” OCR largest box â†’ speak label text once

### App
- [x] **`main.py`** â€” fully voice-controlled webcam loop, OpenCV overlay
- [x] **`app.py`** â€” optional Flask REST API (`/find`, `/read`, `/status`)

---

## ðŸ”œ Not Yet Done / To-Do

### Accuracy & Robustness
- [ ] **Offline speech recognition** â€” currently uses Google Speech API (internet required); swap to Whisper or Vosk for offline use
- [ ] **Vertical guidance** â€” currently only left/right/forward; no up/down guidance (e.g. "look higher", "it's on the bottom shelf")
- [ ] **Multi-target disambiguation** â€” if two "milk" cartons are visible, pick the closer one using depth
- [ ] **Re-lock after occlusion** â€” if IoU tracker loses the target completely, re-trigger OCR search
- [ ] **Confidence-gated OCR** â€” skip OCR if YOLO confidence < threshold (reduce false positives)

### User Experience
- [ ] **Wake word** â€” say "hey milk" to activate, instead of always listening
- [ ] **Earpiece / headphone mode** â€” suppress OpenCV window, audio-only output for real device use
- [ ] **Battery / speed mode toggle** â€” skip MiDaS on low-power mode, use bbox-area only

### Platform
- [ ] **iOS / Android front-end** â€” connect to `app.py` Flask API from a mobile app

---

## Architecture

```
Webcam (OpenCV)
      â†“
YOLOv8n (detect objects)            vision/yolo.py
      â†“
Select top 1â€“2 boxes by confidence
      â†“
MiDaS Depth Estimator               vision/depth.py  
      â†“
PaddleOCR (read text on crop)       vision/ocr.py
      â†“
Keyword match                       logic/match.py
      â†“
IoU Tracker (lock target)           logic/tracker.py
      â†“
Direction (left/right + depth)      logic/direction.py
      â†“
ElevenLabs TTS (throttled)          utils/tts.py
      â†‘
Voice Commands (mic thread)         utils/speech.py
```

---

## Project Structure

```
wheredamilk/
â”œâ”€â”€ main.py              â† voice-controlled webcam loop
â”œâ”€â”€ app.py               â† Flask REST API (optional)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ vision/
â”‚   â”œâ”€â”€ yolo.py          â† YOLOv8n detector
â”‚   â”œâ”€â”€ ocr.py           â† PaddleOCR wrapper
â”‚   â””â”€â”€ depth.py         â† MiDaS monocular depth  â† NEW
â”‚
â”œâ”€â”€ logic/
â”‚   â”œâ”€â”€ match.py         â† keyword matching
â”‚   â”œâ”€â”€ direction.py     â† left/right/ahead + depth
â”‚   â””â”€â”€ tracker.py       â† IoU single-target tracker
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ tts.py           â† ElevenLabs + pyttsx3 fallback
    â””â”€â”€ speech.py        â† continuous mic listener
```

---

## Installation

```bash
cd /Users/athulyaanil/wheredamilk
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# macOS mic support
brew install portaudio && pip install pyaudio

# MiDaS depth model (~400 MB, downloads on first run automatically)
# Nothing extra to install â€” handled by: pip install transformers timm torch
```

---

## Environment Variables

| Variable | Required | Description |
|---|---|---|
| `ELEVEN_API_KEY` | âœ… For best voice | ElevenLabs API key â€” get at [elevenlabs.io](https://elevenlabs.io) |
| `ELEVEN_VOICE_ID` | Optional | Voice name (default: `Rachel`) |

```bash
export ELEVEN_API_KEY="sk-..."
export ELEVEN_VOICE_ID="Rachel"   # or Bella, Antoni, etc.
```

---

## Usage

### Standalone

```bash
python main.py
```

| Say | Action |
|---|---|
| `"find milk"` | Scans, locks on, speaks live directions until you reach it |
| `"find orange juice"` | Works for any item |
| `"read"` / `"what is this"` | OCR the biggest thing in view, speak label once |
| `"stop"` / `"cancel"` | Return to idle |
| `"quit"` / `"exit"` | Close app |

Press `q` in the OpenCV window to also quit.

### Flask API

```bash
python app.py

curl -X POST http://localhost:5000/find -d '{"query":"milk"}' -H 'Content-Type: application/json'
curl -X POST http://localhost:5000/read
curl http://localhost:5000/status
```

---

## Tech Stack

| Library | Purpose | Status |
|---|---|---|
| `ultralytics` | YOLOv8n detection | âœ… Done |
| `opencv-python` | Webcam + drawing | âœ… Done |
| `paddleocr` | Text recognition | âœ… Done |
| `transformers` + `timm` | MiDaS depth model | âœ… Done |
| `elevenlabs` | Natural TTS | âœ… Done |
| `pyttsx3` | Offline TTS fallback | âœ… Done |
| `SpeechRecognition` | Mic voice commands | âœ… Done |
| `flask` | Optional REST API | âœ… Done |
